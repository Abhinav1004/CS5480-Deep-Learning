{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ECLl34A2IHg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ECLl34A2IHg",
    "outputId": "63cdb8e1-2fef-4572-aaf7-419e0a4eb95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fd715-3484-4b4c-bf5e-5028b8889ef8",
   "metadata": {
    "id": "d24fd715-3484-4b4c-bf5e-5028b8889ef8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as utils\n",
    "import torchinfo\n",
    "\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ed689-c5f4-4086-9b45-24309832fcd1",
   "metadata": {
    "id": "413ed689-c5f4-4086-9b45-24309832fcd1"
   },
   "source": [
    "# Question 1\n",
    "\n",
    "**Self-Attention for Object Recognition with CNNs**: Implement a sample CNN with one or more self-attention layer(s) for performing object recognition over CIFAR-10 dataset. You have to implement the self-attention layer yourself and use it in the forward function defined by you. All other layers (fully connected, nonlinearity, conv layer, etc.) can be bulit-in implementations. The network can be a simpler one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ad5d92",
   "metadata": {
    "cellView": "form",
    "id": "61ad5d92"
   },
   "outputs": [],
   "source": [
    "def prepare_data(batch_size, num_workers):\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=train_transform)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=test_transform)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f38fd60-609d-415c-8d3c-ed4cd82275b0",
   "metadata": {
    "id": "8f38fd60-609d-415c-8d3c-ed4cd82275b0"
   },
   "outputs": [],
   "source": [
    "save_path = os.getcwd()\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d024b94-49a9-4233-8435-d8fa63e662e7",
   "metadata": {
    "id": "8d024b94-49a9-4233-8435-d8fa63e662e7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Attention blocks\n",
    "Reference: Learn To Pay Attention Research Paper\n",
    "\n",
    "Idea: To use the architecture as used in above paper with reduced parameters so as to achieve suitable performance.\n",
    "\"\"\"\n",
    "\n",
    "class ProjectorBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ProjectorBlock, self).__init__()\n",
    "        self.op = nn.Conv2d(in_channels=in_features, out_channels=out_features,\n",
    "            kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_features, normalize_attn=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.op = nn.Conv2d(in_channels=in_features, out_channels=1,\n",
    "            kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, l, g):\n",
    "        N, C, H, W = l.size()\n",
    "        c = self.op(l+g)\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,H,W)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        g = torch.mul(a.expand_as(l), l)\n",
    "        if self.normalize_attn:\n",
    "            g = g.view(N,C,-1).sum(dim=2) # (batch_size,C)\n",
    "        else:\n",
    "            g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
    "        return c.view(N,1,H,W), g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9896614c-ba9d-47d0-9cf9-308b036879fb",
   "metadata": {
    "id": "9896614c-ba9d-47d0-9cf9-308b036879fb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Attention Block\n",
    "\"\"\"\n",
    "class CnnAttention(nn.Module):\n",
    "    def __init__(self, sample_size, num_classes, attention=True, normalize_attn=True, init_weights=True):\n",
    "        super(CnnAttention, self).__init__()\n",
    "         # conv blocks\n",
    "        self.conv1 = self._make_layer(3, 16, 1)\n",
    "        self.conv2 = self._make_layer(16, 32, 1)\n",
    "        self.conv3 = self._make_layer(32, 64, 1)\n",
    "        self.conv4 = self._make_layer(64, 64, 1)\n",
    "        self.conv5 = self._make_layer(64, 64, 2, pool=True)\n",
    "        self.dense = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=int(sample_size/32), padding=0, bias=True)\n",
    "        # attention blocks\n",
    "        self.attention = attention\n",
    "        if self.attention:\n",
    "            self.projector = ProjectorBlock(32, 64)\n",
    "            self.attn1 = AttentionBlock(in_features=64, normalize_attn=normalize_attn)\n",
    "            self.attn2 = AttentionBlock(in_features=64, normalize_attn=normalize_attn)\n",
    "            self.attn3 = AttentionBlock(in_features=64, normalize_attn=normalize_attn)\n",
    "        # final classification layer\n",
    "        if self.attention:\n",
    "            self.classify = nn.Linear(in_features=64*3, out_features=num_classes, bias=True)\n",
    "        else:\n",
    "            self.classify = nn.Linear(in_features=64, out_features=num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        l1 = self.conv2(x)\n",
    "        x = F.max_pool2d(l1, kernel_size=2, stride=2, padding=0)\n",
    "        l2 = self.conv3(x)\n",
    "        x = F.max_pool2d(l2, kernel_size=2, stride=2, padding=0)\n",
    "        l3 = self.conv4(x)\n",
    "        x = F.max_pool2d(l3, kernel_size=2, stride=2, padding=0)\n",
    "        x = self.conv5(x)\n",
    "        g = self.dense(x)\n",
    "        # attention\n",
    "        if self.attention:\n",
    "            c1, g1 = self.attn1(self.projector(l1), g)\n",
    "            c2, g2 = self.attn2(l2, g)\n",
    "            c3, g3 = self.attn3(l3, g)\n",
    "            g = torch.cat((g1,g2,g3), dim=1) # batch_sizex3C\n",
    "            # classification layer\n",
    "            x = self.classify(g) # batch_sizexnum_classes\n",
    "        else:\n",
    "            c1, c2, c3 = None, None, None\n",
    "            x = self.classify(torch.squeeze(g))\n",
    "        return [x, c1, c2, c3]\n",
    "\n",
    "    def _make_layer(self, in_features, out_features, blocks, pool=False):\n",
    "        layers = []\n",
    "        for i in range(blocks):\n",
    "            conv2d = nn.Conv2d(in_channels=in_features, out_channels=out_features, kernel_size=3, padding=1, bias=False)\n",
    "            layers += [conv2d, nn.BatchNorm2d(out_features), nn.ReLU(inplace=True)]\n",
    "            in_features = out_features\n",
    "            if pool:\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318d5f8d-93ad-442c-9d63-aa0124ee1316",
   "metadata": {
    "id": "318d5f8d-93ad-442c-9d63-aa0124ee1316"
   },
   "outputs": [],
   "source": [
    "class AttentionTrainer():\n",
    "\n",
    "    def train_epoch(self,model, loss_function, optimizer, dataloader,epoch):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        all_label = []\n",
    "        all_pred = []\n",
    "\n",
    "        # Initialize tqdm to visualize progress\n",
    "        pbar = tqdm(total=len(dataloader), desc=f'Epoch {epoch+1}', unit='batch')\n",
    "\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            # get the inputs and labels\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, list):\n",
    "                outputs = outputs[0]\n",
    "\n",
    "            # compute the loss\n",
    "            loss = loss_function(outputs, labels.squeeze())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # compute the accuracy\n",
    "            prediction = torch.max(outputs, 1)[1]\n",
    "            all_label.extend(labels.squeeze())\n",
    "            all_pred.extend(prediction)\n",
    "            score = accuracy_score(labels.squeeze().cpu().data.squeeze().numpy(), prediction.cpu().data.squeeze().numpy())\n",
    "\n",
    "            # backward & optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)  # Update progress bar\n",
    "\n",
    "        pbar.close()  # Close progress bar after epoch completion\n",
    "\n",
    "        # Compute the average loss & accuracy\n",
    "        training_loss = sum(losses)/len(losses)\n",
    "        all_label = torch.stack(all_label, dim=0)\n",
    "        all_pred = torch.stack(all_pred, dim=0)\n",
    "        training_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "        print(\"Training Loss: {:.6f} | Training Accuracy: {:.2f}%\".format(training_loss, training_acc*100))\n",
    "\n",
    "    def val_epoch(self,model, loss_function, dataloader,epoch):\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        all_label = []\n",
    "        all_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "                # get the inputs and labels\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, list):\n",
    "                    outputs = outputs[0]\n",
    "                # compute the loss\n",
    "                loss = loss_function(outputs, labels.squeeze())\n",
    "                losses.append(loss.item())\n",
    "                # collect labels & prediction\n",
    "                prediction = torch.max(outputs, 1)[1]\n",
    "                all_label.extend(labels.squeeze())\n",
    "                all_pred.extend(prediction)\n",
    "\n",
    "        # Compute the average loss & accuracy\n",
    "        val_loss = sum(losses)/len(losses)\n",
    "        all_label = torch.stack(all_label, dim=0)\n",
    "        all_pred = torch.stack(all_pred, dim=0)\n",
    "        val_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
    "        print(\"Validation Loss: {:.6f} | Validation Accuracy: {:.2f}%\".format(val_loss, val_acc*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233d19c0-a4c3-4ded-b648-8d12d623d2c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "233d19c0-a4c3-4ded-b648-8d12d623d2c4",
    "outputId": "b7ce6e1b-c89f-4429-d6dd-d32cb715a0d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "CnnAttention                             [1, 10]                   --\n",
      "├─Sequential: 1-1                        [1, 16, 32, 32]           --\n",
      "│    └─Conv2d: 2-1                       [1, 16, 32, 32]           432\n",
      "│    └─BatchNorm2d: 2-2                  [1, 16, 32, 32]           32\n",
      "│    └─ReLU: 2-3                         [1, 16, 32, 32]           --\n",
      "├─Sequential: 1-2                        [1, 32, 32, 32]           --\n",
      "│    └─Conv2d: 2-4                       [1, 32, 32, 32]           4,608\n",
      "│    └─BatchNorm2d: 2-5                  [1, 32, 32, 32]           64\n",
      "│    └─ReLU: 2-6                         [1, 32, 32, 32]           --\n",
      "├─Sequential: 1-3                        [1, 64, 16, 16]           --\n",
      "│    └─Conv2d: 2-7                       [1, 64, 16, 16]           18,432\n",
      "│    └─BatchNorm2d: 2-8                  [1, 64, 16, 16]           128\n",
      "│    └─ReLU: 2-9                         [1, 64, 16, 16]           --\n",
      "├─Sequential: 1-4                        [1, 64, 8, 8]             --\n",
      "│    └─Conv2d: 2-10                      [1, 64, 8, 8]             36,864\n",
      "│    └─BatchNorm2d: 2-11                 [1, 64, 8, 8]             128\n",
      "│    └─ReLU: 2-12                        [1, 64, 8, 8]             --\n",
      "├─Sequential: 1-5                        [1, 64, 1, 1]             --\n",
      "│    └─Conv2d: 2-13                      [1, 64, 4, 4]             36,864\n",
      "│    └─BatchNorm2d: 2-14                 [1, 64, 4, 4]             128\n",
      "│    └─ReLU: 2-15                        [1, 64, 4, 4]             --\n",
      "│    └─MaxPool2d: 2-16                   [1, 64, 2, 2]             --\n",
      "│    └─Conv2d: 2-17                      [1, 64, 2, 2]             36,864\n",
      "│    └─BatchNorm2d: 2-18                 [1, 64, 2, 2]             128\n",
      "│    └─ReLU: 2-19                        [1, 64, 2, 2]             --\n",
      "│    └─MaxPool2d: 2-20                   [1, 64, 1, 1]             --\n",
      "├─Conv2d: 1-6                            [1, 64, 1, 1]             4,160\n",
      "├─ProjectorBlock: 1-7                    [1, 64, 32, 32]           --\n",
      "│    └─Conv2d: 2-21                      [1, 64, 32, 32]           2,048\n",
      "├─AttentionBlock: 1-8                    [1, 1, 32, 32]            --\n",
      "│    └─Conv2d: 2-22                      [1, 1, 32, 32]            64\n",
      "├─AttentionBlock: 1-9                    [1, 1, 16, 16]            --\n",
      "│    └─Conv2d: 2-23                      [1, 1, 16, 16]            64\n",
      "├─AttentionBlock: 1-10                   [1, 1, 8, 8]              --\n",
      "│    └─Conv2d: 2-24                      [1, 1, 8, 8]              64\n",
      "├─Linear: 1-11                           [1, 10]                   1,930\n",
      "==========================================================================================\n",
      "Total params: 143,002\n",
      "Trainable params: 143,002\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 15.17\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.67\n",
      "Params size (MB): 0.57\n",
      "Estimated Total Size (MB): 2.25\n",
      "==========================================================================================\n",
      "Total number of parameters in CNN with Attention Model is : 143002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CIFAR10 dataset\n",
    "trainloader, testloader = prepare_data(batch_size=batch_size,num_workers=2)\n",
    "\n",
    "# Create model\n",
    "model_cnn_attention = CnnAttention(sample_size=32, num_classes=10).to(device)\n",
    "\n",
    "print(torchinfo.summary(model_cnn_attention, (1, 3, 32, 32)))\n",
    "\n",
    "\n",
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in model_cnn_attention.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in CNN with Attention Model is :\", total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb30859-dfb0-4ba5-b446-936ad8e49c9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bb30859-dfb0-4ba5-b446-936ad8e49c9f",
    "outputId": "59782599-9c81-46b8-f38c-562a799212f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 1: 100%|██████████| 1563/1563 [00:49<00:00, 31.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.679620 | Training Accuracy: 39.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.443048 | Validation Accuracy: 47.39%\n",
      "Saving Model of Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 2: 100%|█████████▉| 1561/1563 [00:42<00:00, 28.88batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 2: 100%|██████████| 1563/1563 [00:42<00:00, 36.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.355358 | Training Accuracy: 51.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.287838 | Validation Accuracy: 53.61%\n",
      "Saving Model of Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 3:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 3: 100%|█████████▉| 1561/1563 [00:41<00:00, 42.54batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 3: 100%|██████████| 1563/1563 [00:41<00:00, 37.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.227489 | Training Accuracy: 56.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.170680 | Validation Accuracy: 58.18%\n",
      "Saving Model of Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 4:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 4: 100%|██████████| 1563/1563 [00:42<00:00, 36.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.148179 | Training Accuracy: 59.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.116957 | Validation Accuracy: 60.31%\n",
      "Saving Model of Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 5:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 5: 100%|██████████| 1563/1563 [00:40<00:00, 38.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.086726 | Training Accuracy: 62.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.052005 | Validation Accuracy: 62.46%\n",
      "Saving Model of Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 6:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 6: 100%|█████████▉| 1558/1563 [00:41<00:00, 39.47batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 6: 100%|██████████| 1563/1563 [00:41<00:00, 37.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.036030 | Training Accuracy: 63.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.000039 | Validation Accuracy: 64.47%\n",
      "Saving Model of Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 7:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 7: 100%|██████████| 1563/1563 [00:40<00:00, 38.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.992370 | Training Accuracy: 65.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.962337 | Validation Accuracy: 65.34%\n",
      "Saving Model of Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 8:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 8: 100%|██████████| 1563/1563 [00:40<00:00, 26.92batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 8: 100%|██████████| 1563/1563 [00:40<00:00, 38.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.960051 | Training Accuracy: 66.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.980813 | Validation Accuracy: 65.11%\n",
      "Saving Model of Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 9:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 9: 100%|█████████▉| 1561/1563 [00:40<00:00, 38.15batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 9: 100%|██████████| 1563/1563 [00:40<00:00, 38.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.923124 | Training Accuracy: 67.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.922379 | Validation Accuracy: 67.07%\n",
      "Saving Model of Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 10:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 10: 100%|█████████▉| 1558/1563 [00:40<00:00, 39.81batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 10: 100%|██████████| 1563/1563 [00:40<00:00, 38.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.900278 | Training Accuracy: 68.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.914685 | Validation Accuracy: 67.64%\n",
      "Saving Model of Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 11:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 11: 100%|█████████▉| 1560/1563 [00:41<00:00, 26.22batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 11: 100%|██████████| 1563/1563 [00:41<00:00, 37.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.876151 | Training Accuracy: 69.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.881864 | Validation Accuracy: 68.84%\n",
      "Saving Model of Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 12:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 12: 100%|██████████| 1563/1563 [00:40<00:00, 45.34batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 12: 100%|██████████| 1563/1563 [00:40<00:00, 38.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.851608 | Training Accuracy: 70.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.862305 | Validation Accuracy: 68.97%\n",
      "Saving Model of Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 13:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 13: 100%|█████████▉| 1558/1563 [00:41<00:00, 39.66batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 13: 100%|██████████| 1563/1563 [00:41<00:00, 37.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.834642 | Training Accuracy: 71.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.853907 | Validation Accuracy: 70.11%\n",
      "Saving Model of Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 14:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 14: 100%|██████████| 1563/1563 [00:40<00:00, 38.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.811061 | Training Accuracy: 72.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.810909 | Validation Accuracy: 71.38%\n",
      "Saving Model of Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 15:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 15: 100%|██████████| 1563/1563 [00:40<00:00, 38.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.798187 | Training Accuracy: 72.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.809096 | Validation Accuracy: 71.91%\n",
      "Saving Model of Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 16:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 16: 100%|█████████▉| 1560/1563 [00:42<00:00, 25.68batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 16: 100%|██████████| 1563/1563 [00:42<00:00, 36.90batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.783240 | Training Accuracy: 72.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.804517 | Validation Accuracy: 71.81%\n",
      "Saving Model of Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 17:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 17: 100%|█████████▉| 1559/1563 [00:42<00:00, 41.28batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 17: 100%|██████████| 1563/1563 [00:42<00:00, 36.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.767560 | Training Accuracy: 73.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.775583 | Validation Accuracy: 73.11%\n",
      "Saving Model of Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 18:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 18: 100%|█████████▉| 1562/1563 [00:42<00:00, 42.37batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 18: 100%|██████████| 1563/1563 [00:42<00:00, 36.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.755890 | Training Accuracy: 73.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.771936 | Validation Accuracy: 73.23%\n",
      "Saving Model of Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 19:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 19: 100%|█████████▉| 1561/1563 [00:43<00:00, 37.45batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 19: 100%|██████████| 1563/1563 [00:44<00:00, 35.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.740731 | Training Accuracy: 74.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.761120 | Validation Accuracy: 73.72%\n",
      "Saving Model of Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 20:   0%|          | 0/1563 [00:00<?, ?batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 20: 100%|█████████▉| 1559/1563 [00:42<00:00, 38.61batch/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 20: 100%|██████████| 1563/1563 [00:43<00:00, 36.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.729246 | Training Accuracy: 74.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.733077 | Validation Accuracy: 74.47%\n",
      "Saving Model of Epoch 20\n"
     ]
    }
   ],
   "source": [
    "# Define loss function & optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn_attention.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "cnn_attention_trainer = AttentionTrainer()\n",
    "for epoch in range(epochs):\n",
    "    cnn_attention_trainer.train_epoch(model_cnn_attention, loss_function, optimizer, trainloader, epoch)\n",
    "    cnn_attention_trainer.val_epoch(model_cnn_attention, loss_function, testloader, epoch)\n",
    "\n",
    "    torch.save(model_cnn_attention.state_dict(), os.path.join(save_path, \"cnn_epoch{:03d}.pth\".format(epoch+1)))\n",
    "    print(\"Saving Model of Epoch {}\".format(epoch+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc5473",
   "metadata": {
    "id": "67cc5473"
   },
   "source": [
    "# Question-2 ViT Implementation\n",
    "\n",
    "Object Recognition with Vision Transformer: Implement and train an Encoder only Transformer (ViT-like) for the above object recognition task. In other words, implement multi-headed self-attention for the image classification (i.e., appending a < class > token to the image patches that are accepted as input tokens). Compare the performance of the two implementations (try to keep the number of parameters to be comparable and use the same amount of training and testing data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5757d1a5",
   "metadata": {
    "cellView": "form",
    "id": "5757d1a5"
   },
   "outputs": [],
   "source": [
    "class GELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Gaussian Error Linear Unit (GELU) activation function.\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class ImagePatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert images into patches and project them into a vector space.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchAndPositionEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines patch embeddings with positional embeddings and class token.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = ImagePatchEmbeddings(config)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e81378a-7907-4ec2-8b5b-e02e32c22a3a",
   "metadata": {
    "id": "0e81378a-7907-4ec2-8b5b-e02e32c22a3a"
   },
   "outputs": [],
   "source": [
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Single attention head module.\n",
    "    Used in Multi-Head Attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    Used in Transformer Encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37282cad-fdb4-40c3-95ce-f2af231ba6b8",
   "metadata": {
    "id": "37282cad-fdb4-40c3-95ce-f2af231ba6b8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron module.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = GELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer block module.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        attention_output, attention_probs = self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
    "        x = x + attention_output\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        x = x + mlp_output\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = TransformerBlock(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "789a4109-bdd7-4ff8-9f32-9d3f90130cfa",
   "metadata": {
    "id": "789a4109-bdd7-4ff8-9f32-9d3f90130cfa"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ViTForClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer model for classification tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.embedding = PatchAndPositionEmbeddings(config)\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        embedding_output = self.embedding(x)\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "        logits = self.classifier(encoder_output[:, 0, :])\n",
    "        if not output_attentions:\n",
    "            return (logits, None)\n",
    "        else:\n",
    "            return (logits, all_attentions)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, PatchAndPositionEmbeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b37e9293",
   "metadata": {
    "id": "b37e9293"
   },
   "outputs": [],
   "source": [
    "exp_name = 'vit-model'\n",
    "batch_size = 256\n",
    "epochs = 20\n",
    "lr = 0.01\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = {\n",
    "    \"patch_size\": 4,\n",
    "    \"hidden_size\": 48,\n",
    "    \"num_hidden_layers\": 4,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"intermediate_size\": 4 * 48,\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 32,\n",
    "    \"num_classes\": 10,\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b67b470-fd43-4152-b94b-8f7e7f197681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b67b470-fd43-4152-b94b-8f7e7f197681",
    "outputId": "ba7fafee-69a4-400b-e5b0-11aa270bccfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "ViTForClassification                               [1, 10]                   --\n",
      "├─PatchAndPositionEmbeddings: 1-1                  [1, 65, 48]               3,168\n",
      "│    └─ImagePatchEmbeddings: 2-1                   [1, 64, 48]               --\n",
      "│    │    └─Conv2d: 3-1                            [1, 48, 8, 8]             2,352\n",
      "│    └─Dropout: 2-2                                [1, 65, 48]               --\n",
      "├─TransformerEncoder: 1-2                          [1, 65, 48]               --\n",
      "│    └─ModuleList: 2-3                             --                        --\n",
      "│    │    └─TransformerBlock: 3-2                  [1, 65, 48]               28,272\n",
      "│    │    └─TransformerBlock: 3-3                  [1, 65, 48]               28,272\n",
      "│    │    └─TransformerBlock: 3-4                  [1, 65, 48]               28,272\n",
      "│    │    └─TransformerBlock: 3-5                  [1, 65, 48]               28,272\n",
      "├─Linear: 1-3                                      [1, 10]                   490\n",
      "====================================================================================================\n",
      "Total params: 119,098\n",
      "Trainable params: 119,098\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.26\n",
      "====================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.12\n",
      "Params size (MB): 0.46\n",
      "Estimated Total Size (MB): 1.60\n",
      "====================================================================================================\n",
      "Total number of parameters: 119098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 1:  99%|█████████▉| 194/196 [00:35<00:00, 18.45it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Epoch 1: 100%|██████████| 196/196 [00:35<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.6875, Train Accuracy: 0.3707             Test loss: 1.6813,Test Accuracy: 0.3680, Time Taken: 68.83290410041809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 196/196 [00:35<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 1.4890, Train Accuracy: 0.4550             Test loss: 1.4813,Test Accuracy: 0.4655, Time Taken: 68.7428731918335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 196/196 [00:34<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 1.4079, Train Accuracy: 0.4880             Test loss: 1.4180,Test Accuracy: 0.4870, Time Taken: 69.29568433761597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 196/196 [00:33<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.4366, Train Accuracy: 0.4690             Test loss: 1.4134,Test Accuracy: 0.4764, Time Taken: 68.3382019996643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 196/196 [00:34<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 1.2800, Train Accuracy: 0.5382             Test loss: 1.2980,Test Accuracy: 0.5342, Time Taken: 67.98331594467163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 196/196 [00:34<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 1.2287, Train Accuracy: 0.5543             Test loss: 1.2502,Test Accuracy: 0.5501, Time Taken: 67.30260753631592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 196/196 [00:34<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 1.2589, Train Accuracy: 0.5488             Test loss: 1.2801,Test Accuracy: 0.5441, Time Taken: 67.425119638443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 196/196 [00:34<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 1.2002, Train Accuracy: 0.5608             Test loss: 1.2008,Test Accuracy: 0.5652, Time Taken: 68.62580943107605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 196/196 [00:33<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 1.1651, Train Accuracy: 0.5810             Test loss: 1.1893,Test Accuracy: 0.5725, Time Taken: 67.5715742111206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 196/196 [00:34<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 1.1399, Train Accuracy: 0.5903             Test loss: 1.1615,Test Accuracy: 0.5764, Time Taken: 67.3302161693573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 196/196 [00:34<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 1.1386, Train Accuracy: 0.5922             Test loss: 1.1667,Test Accuracy: 0.5744, Time Taken: 67.43823504447937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 196/196 [00:34<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 1.1150, Train Accuracy: 0.6047             Test loss: 1.2042,Test Accuracy: 0.5774, Time Taken: 67.38134789466858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 196/196 [00:34<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 1.1119, Train Accuracy: 0.5987             Test loss: 1.1399,Test Accuracy: 0.5927, Time Taken: 68.9344527721405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 196/196 [00:33<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train loss: 1.0876, Train Accuracy: 0.6148             Test loss: 1.1054,Test Accuracy: 0.6087, Time Taken: 67.57394289970398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 196/196 [00:33<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 1.0656, Train Accuracy: 0.6184             Test loss: 1.1390,Test Accuracy: 0.5947, Time Taken: 67.3763952255249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 196/196 [00:34<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train loss: 1.0271, Train Accuracy: 0.6353             Test loss: 1.0755,Test Accuracy: 0.6173, Time Taken: 67.2056360244751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 196/196 [00:34<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train loss: 1.0110, Train Accuracy: 0.6408             Test loss: 1.0587,Test Accuracy: 0.6251, Time Taken: 67.86719822883606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 196/196 [00:34<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train loss: 0.9653, Train Accuracy: 0.6550             Test loss: 1.0348,Test Accuracy: 0.6274, Time Taken: 68.74163913726807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 196/196 [00:33<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train loss: 0.9554, Train Accuracy: 0.6602             Test loss: 1.0126,Test Accuracy: 0.6359, Time Taken: 67.89038300514221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 196/196 [00:33<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train loss: 0.9641, Train Accuracy: 0.6536             Test loss: 1.0199,Test Accuracy: 0.6315, Time Taken: 67.724360704422\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    The simple trainer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, loss_fn, exp_name, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.exp_name = exp_name\n",
    "        self.device = device\n",
    "\n",
    "    def train_epoch(self, trainloader,epoch_num):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        progress_bar = tqdm(enumerate(trainloader), total=len(trainloader))\n",
    "\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            # Move the batch to the device\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            images, labels = batch\n",
    "            # Zero the gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            # Calculate the loss\n",
    "            loss = self.loss_fn(self.model(images)[0], labels)\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "            # Update the model's parameters\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item() * len(images)\n",
    "\n",
    "            progress_bar.set_description(f'Epoch {epoch_num}')\n",
    "\n",
    "        return total_loss / len(trainloader.dataset)\n",
    "\n",
    "\n",
    "    def train(self, trainloader, testloader, epochs):\n",
    "        \"\"\"\n",
    "        Train the model for the specified number of epochs.\n",
    "        \"\"\"\n",
    "        # Keep track of the losses and accuracies\n",
    "        train_losses, test_losses, train_accuracies,test_accuracies = [], [], [],[]\n",
    "        # Train the model\n",
    "        for i in range(epochs):\n",
    "            start_time = time()\n",
    "            train_loss = self.train_epoch(trainloader,i+1)\n",
    "\n",
    "            train_accuracy, train_loss = self.evaluate(trainloader)\n",
    "            test_accuracy, test_loss = self.evaluate(testloader)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "            end_time = time()\n",
    "\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} \\\n",
    "            Test loss: {test_loss:.4f},Test Accuracy: {test_accuracy:.4f}, Time Taken: {end_time-start_time}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                # Get predictions\n",
    "                logits, _ = self.model(images)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def vit_main():\n",
    "    # Load the CIFAR10 dataset\n",
    "    trainloader, testloader = prepare_data(batch_size=batch_size,num_workers=10)\n",
    "\n",
    "\n",
    "    # Create the model, optimizer, loss function and trainer\n",
    "    model_vit = ViTForClassification(config)\n",
    "    print(torchinfo.summary(model_vit, (1, 3, 32, 32)))\n",
    "\n",
    "    # Calculate the total number of parameters\n",
    "    total_params = sum(p.numel() for p in model_vit.parameters() if p.requires_grad)\n",
    "    print(\"Total number of parameters:\", total_params)\n",
    "\n",
    "    optimizer = optim.AdamW(model_vit.parameters(), lr=lr, weight_decay=0.03)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model_vit, optimizer, loss_fn, exp_name, device=device)\n",
    "    trainer.train(trainloader, testloader, epochs)\n",
    "\n",
    "\n",
    "vit_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e5cd8-b96c-4ad8-b25c-95b4a666adaf",
   "metadata": {
    "id": "651e5cd8-b96c-4ad8-b25c-95b4a666adaf"
   },
   "source": [
    "# Observations\n",
    "\n",
    "1. CNN with self attention performs better compared to ViT for the above Cifar10 dataset.\n",
    "2. It should be also noted that ViT performance improves significanly for larger datasets and it overcomes the results produced by CNN by a considerable margin.\n",
    "\n",
    "\n",
    "| Model Name | Training Accuracy | Validation Accuracy |  Number of Epochs |  Number of Parameters |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| CNN with Self Attention | 80.16 | 78.89 | 50 | 143002 |\n",
    "| ViT | 75.45 | 71.76 | 50 | 119098 |\n",
    "\n",
    "\n",
    "For the above notebook to be trainable on my local machine(Mac M1 Pro with 16GB RAM) i have constrained the number of epochs so as to get the results faster with 20 epochs and below are the corresponding observations.\n",
    "\n",
    "\n",
    "\n",
    "| Model Name | Training Accuracy | Validation Accuracy |  Number of Epochs |Number of Parameters |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| CNN with Self Attention | 74.79 | 73.49 | 20 |143002 |\n",
    "| ViT | 64.19 | 62.65 | 20 |119098 |\n",
    "\n",
    "\n",
    "**Note**:\n",
    "```\n",
    "1. The above produced metrics are averaged over 5 complete run  \n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIvJcyQTDvb",
   "metadata": {
    "id": "FSIvJcyQTDvb"
   },
   "source": [
    "# Reference Papers\n",
    "**For Question 1**\n",
    "1. https://arxiv.org/pdf/1804.02391.pdf Learn to pay attention\n",
    "2. https://arxiv.org/pdf/2111.14556.pdf On the Integration of Self-Attention and Convolution\n",
    "\n",
    "**Question 2**\n",
    "1. https://arxiv.org/pdf/2302.03751.pdf Understanding Why ViT Trains Badly on Small\n",
    "Datasets: An Intuitive Perspective\n",
    "2. https://arxiv.org/pdf/2010.11929.pdf AN IMAGE IS WORTH 16X16 WORDS:\n",
    "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0KUqNwmS-Sf",
   "metadata": {
    "id": "d0KUqNwmS-Sf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1907a-4dd6-4427-99c7-789402f9d02f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
